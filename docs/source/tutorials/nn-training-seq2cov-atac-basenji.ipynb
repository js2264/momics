{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict genomic coverage from DNA sequence\n",
    "\n",
    "Because `momics` can ingest reference genome sequence as well as genomic coverage data, we can use it to predict genomic coverage from DNA sequence. This is useful for generating synthetic data or for filling in missing data in a dataset.\n",
    "\n",
    "## Connect to the data repository\n",
    "\n",
    "Here again, we will tap into the repository generated in the [previous tutorial](integrating-multiomics.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momics.momics import Momics\n",
    "\n",
    "## Creating repository\n",
    "repo = Momics(\"yeast_CNN_data.momics\")\n",
    "\n",
    "## Check that sequence and some tracks are registered\n",
    "repo.seq()\n",
    "repo.tracks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define datasets and model \n",
    "\n",
    "We will define a simple convolutional neural network with `tensorflow` to predict the target variable `mnase` from the feature variable `seq` (the genome reference sequence). This requires to first define a set of genomic coordinates to extract genomic data from. We will extract sequences over tiling genomic windows (`features_size` of `8193`, with a stride of `48`) as feature variables to predict `mnase_rescaled` coverage scores over the same tiling genomic windows, but narrowed down to the a `target_size` of `128` bp around the center of the window. We can split the data into training, testing and validation sets, using `momics.utils.split_ranges()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import momics.utils as mutils\n",
    "from momics.dataset import MomicsDataset\n",
    "\n",
    "# Fetch data from the momics repository\n",
    "features = \"nucleotide\"\n",
    "target = \"atac_rescaled\"\n",
    "features_size = 8192 + 1\n",
    "stride = 48\n",
    "target_size = 512\n",
    "batch_size = 500\n",
    "\n",
    "bins = repo.bins(width=features_size, stride=stride, cut_last_bin_out=True)\n",
    "bins = bins.subset(lambda x: x.Chromosome != \"XVI\")\n",
    "bins_split, bins_test = mutils.split_ranges(bins, 0.8, shuffle=False)\n",
    "bins_train, bins_val = mutils.split_ranges(bins_split, 0.8, shuffle=False)\n",
    "\n",
    "train_dataset = (\n",
    "    MomicsDataset(repo, bins_train, features, target, target_size=target_size, batch_size=batch_size).prefetch(20).repeat()\n",
    ")\n",
    "val_dataset = MomicsDataset(repo, bins_val, features, target, target_size=target_size, batch_size=batch_size).repeat()\n",
    "test_dataset = MomicsDataset(repo, bins_test, features, target, target_size=target_size, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to define the model architecture. In this example, we will use a neural network adapted from `Basenji`, pre-defined in `momics.nn`. We can instantiate the model, and compile it with the desired optimizer, loss function and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momics import nn\n",
    "from momics.nn import mae_cor\n",
    "import tensorflow as tf  # type: ignore\n",
    "from tensorflow.keras import layers  # type: ignore\n",
    "\n",
    "model = nn.Basenji(input=layers.Input(shape=(features_size, 4)), output=layers.Dense(target_size, activation=\"linear\")).model\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    return mae_cor(y_true, y_pred, alpha=0.9)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=loss_fn,\n",
    "    metrics=[\"mae\"],\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the datasets and the model, we can fit the model to the training data, using the `fit()` method of the model. We can also evaluate the model on the testing and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # type: ignore\n",
    "\n",
    "callbacks_list = [\n",
    "    CSVLogger(Path(\".chromnn\", \"seq2cov.basenji.epoch_data.csv\")),\n",
    "    ModelCheckpoint(\n",
    "        filepath=Path(\".chromnn_seq2cov\", \"Checkpoint.seq2cov.basenji.keras\"), monitor=\"val_loss\", save_best_only=True\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=40, min_delta=1e-5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=6 // 2, min_lr=0.1 * 0.001),\n",
    "]\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks_list,\n",
    "    steps_per_epoch=int(np.floor(len(bins_train) // batch_size)),\n",
    "    validation_steps=int(np.floor(len(bins_val) // batch_size)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and save model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the trained model performs, and save it to the local repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model to predict MNase-seq coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our trained model to predict ATAC-seq coverage from MNase-seq coverage, for example on a chromosome which has not been used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momics import dataset as mmd\n",
    "from momics import aggregate as mma\n",
    "\n",
    "## Now predict the ATAC signal from the MNase signal\n",
    "bb = repo.bins(width=features_size, stride=8, cut_last_bin_out=True)[\"XVI\"]\n",
    "ds = mmd.MomicsDataset(repo, bb, \"nucleotide\", batch_size=1000).prefetch(10)\n",
    "predictions = model.predict(ds)\n",
    "\n",
    "## Export predictions as a bigwig\n",
    "bb3 = bb.copy()\n",
    "bb3.Start = bb3.Start + features_size // 2 - target_size // 2\n",
    "bb3.End = bb3.Start + target_size\n",
    "chrom_sizes = {chrom: length for chrom, length in zip(repo.chroms().chrom, repo.chroms().length)}\n",
    "keys = [f\"{chrom}:{start}-{end}\" for chrom, start, end in zip(bb3.Chromosome, bb3.Start, bb3.End)]\n",
    "res = {f\"atac-from-seq-basenji_f{features_size}_s{stride}_t{target_size}\": {k: None for k in keys}}\n",
    "for i, key in enumerate(keys):\n",
    "    res[f\"atac-from-seq-basenji_f{features_size}_s{stride}_t{target_size}\"][key] = predictions[i]\n",
    "\n",
    "mma.aggregate(res, bb3, chrom_sizes, type=\"mean\", prefix=\"prediction\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
