{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train neural networks with `momics` and `tensorflow`\n",
    "\n",
    "`momics` provides several useful resources to train neural networks with `tensorflow`. This notebook demonstrates how to train a simple neural network with `momics` and `tensorflow`.\n",
    "\n",
    "## Connect to the data\n",
    "\n",
    "First, we need to connect to the data. We will reuse the repository from the [previous tutorial](integrating-multiomics). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>/data/momics/S288c_atac.bw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MNase</td>\n",
       "      <td>/data/momics/S288c_mnase.bw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ATAC_rescaled</td>\n",
       "      <td>tmp1ae0oz8z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MNase_rescaled</td>\n",
       "      <td>tmpxo8t09eu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx           label                         path\n",
       "0    0            ATAC   /data/momics/S288c_atac.bw\n",
       "1    1           MNase  /data/momics/S288c_mnase.bw\n",
       "2    2   ATAC_rescaled                  tmp1ae0oz8z\n",
       "3    3  MNase_rescaled                  tmpxo8t09eu"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from momics.momics import Momics\n",
    "\n",
    "## Creating repository\n",
    "repo = Momics(\"yeast_CNN_data.momics\")\n",
    "\n",
    "## Check that sequence and some tracks are registered\n",
    "repo.seq()\n",
    "repo.tracks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify some tracks\n",
    "\n",
    "We can first pre-process the tracks to normalize them, and save them back to the local repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Provided label 'ATAC_rescaled' already present in `tracks` table",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m         arr \u001b[38;5;241m=\u001b[39m (arr \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmin(arr)) \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mnanmax(arr) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmin(arr))\n\u001b[1;32m     10\u001b[0m         cov[chrom] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(arr, nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mingest_track\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_rescaled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m repo\u001b[38;5;241m.\u001b[39mtracks()\n",
      "File \u001b[0;32m~/repos/momics/src/momics/momics.py:849\u001b[0m, in \u001b[0;36mMomics.ingest_track\u001b[0;34m(self, coverage, track, threads)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;66;03m# Abort if bw labels already exist\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m track \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(tracks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided label \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrack\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already present in `tracks` table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    851\u001b[0m \u001b[38;5;66;03m# Save the coverage dict as a temporary bigwig file\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;66;03m# and ingest it using `ingest_tracks`\u001b[39;00m\n\u001b[1;32m    853\u001b[0m tmp_bw \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(delete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Provided label 'ATAC_rescaled' already present in `tracks` table"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for track in [\"ATAC\", \"MNase\"]:\n",
    "    cov = repo.tracks(track)\n",
    "    q99 = np.nanpercentile(np.concatenate(list(cov.values())), 99)\n",
    "    for chrom in cov.keys():\n",
    "        arr = cov[chrom]\n",
    "        arr = np.minimum(arr, q99)\n",
    "        arr = (arr - np.nanmin(arr)) / (np.nanmax(arr) - np.nanmin(arr))\n",
    "        cov[chrom] = np.nan_to_num(arr, nan=0)\n",
    "    repo.ingest_track(cov, track + \"_rescaled\")\n",
    "\n",
    "repo.tracks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model \n",
    "\n",
    "We will define a simple convolutional neural network with `tensorflow` to predict the target variable `ATAC` from the feature variable `MNase`. This requires to first define `MomicsDataset` objects for a training set, a validation set, and a test set. We will use `MNase` coverage scores over tiling genomic windows (width: 2048, stride: 2048) as feature variables to predict `ATAC` coverage scores over the same tiling genomic windows, but narrowed down to the 16bp around the center of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 13:56:19.009933: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-29 13:56:19.021894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738155379.034796  209475 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738155379.038236  209475 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-29 13:56:19.053073: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1738155380.455845  209475 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10014 MB memory:  -> device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:55:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_ZipDataset element_spec=((TensorSpec(shape=(None, 2048, 1), dtype=tf.float32, name=None),), (TensorSpec(shape=(None, 16, 1), dtype=tf.float32, name=None),))>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from momics.dataset import MomicsDataset\n",
    "import momics.utils as mutils\n",
    "\n",
    "# Fetch data from the momics repository\n",
    "features = \"MNase_rescaled\"\n",
    "features_size = 2048\n",
    "target = \"ATAC_rescaled\"\n",
    "target_size = 16\n",
    "stride = 2048\n",
    "batch_size = 1024\n",
    "\n",
    "bins = repo.bins(width=features_size, stride=stride, cut_last_bin_out=True)\n",
    "bins_split, bins_test = mutils.split_ranges(bins, 0.8, shuffle=False)\n",
    "bins_train, bins_val = mutils.split_ranges(bins_split, 0.8, shuffle=True)\n",
    "\n",
    "train_dataset = MomicsDataset(repo, bins_train, features, target, target_size=target_size, batch_size=batch_size)\n",
    "val_dataset = MomicsDataset(repo, bins_val, features, target, target_size=target_size, batch_size=batch_size)\n",
    "test_dataset = MomicsDataset(repo, bins_test, features, target, target_size=target_size, batch_size=batch_size)\n",
    "\n",
    "train_dataset  # noqa: B018\n",
    "val_dataset  # noqa: B018\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The total size of the tensor must be unchanged. Received: input_shape=(16,), target_shape=(1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mChromNN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/repos/momics/src/momics/chromnn.py:35\u001b[0m, in \u001b[0;36mChromNN.__init__\u001b[0;34m(self, input, output)\u001b[0m\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mFlatten()(x)\n\u001b[1;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m output(x)\n\u001b[0;32m---> 35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(\u001b[38;5;28minput\u001b[39m, x)\n",
      "File \u001b[0;32m~/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/ops/operation_utils.py:302\u001b[0m, in \u001b[0;36mcompute_reshape_output_shape\u001b[0;34m(input_shape, newshape, newshape_arg_name)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unknown_dim_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_size \u001b[38;5;241m!=\u001b[39m math\u001b[38;5;241m.\u001b[39mprod(newshape):\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe total size of the tensor must be unchanged. Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnewshape_arg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnewshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         )\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m newshape\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# We have one -1 in `newshape`, compute the actual value\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The total size of the tensor must be unchanged. Received: input_shape=(16,), target_shape=(1, 1)"
     ]
    }
   ],
   "source": [
    "from momics.chromnn import ChromNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers  # type: ignore\n",
    "\n",
    "model = ChromNN(\n",
    "    layers.Input(shape=(features_size, 1)),\n",
    "    layers.Dense(target_size, activation=\"linear\"),\n",
    ").model\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729497492.875502 2553417 service.cc:146] XLA service 0x7fe100009520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1729497492.875515 2553417 service.cc:154]   StreamExecutor device (0): Quadro P400, Compute Capability 6.1\n",
      "2024-10-21 09:58:12.942499: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-21 09:58:13.279690: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2024-10-21 09:58:18.083385: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.70GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-10-21 09:58:21.314785: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng1{k2=2,k3=0} for conv (f32[1,32,1,12]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,1024,1,2059]{3,2,1,0}, f32[32,1024,1,2048]{3,2,1,0}), window={size=1x2048}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-10-21 09:58:21.459192: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.144503023s\n",
      "Trying algorithm eng1{k2=2,k3=0} for conv (f32[1,32,1,12]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,1024,1,2059]{3,2,1,0}, f32[32,1024,1,2048]{3,2,1,0}), window={size=1x2048}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-10-21 09:58:26.161482: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-10-21 09:58:30.266315: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_14', 4 bytes spill stores, 12 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_13', 4 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1729497510.306013 2553417 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453ms/step - loss: 2.2423"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:58:34.701244: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.90GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-10-21 09:58:36.423690: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.58GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3s/step - loss: 2.2155 - val_loss: 0.0260 - learning_rate: 0.0010\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:58:36.827083: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-10-21 09:58:36.827107: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2024-10-21 09:58:36.827115: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "2024-10-21 09:58:36.827128: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n",
      "2024-10-21 09:58:41.179100: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-10-21 09:58:47.463465: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-10-21 09:58:51.825537: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_14', 4 bytes spill stores, 12 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_13', 4 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 15s/step - loss: 1.7935"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:58:52.193159: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2024-10-21 09:58:52.193190: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "2024-10-21 09:58:52.380865: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-10-21 09:58:52.380889: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 117ms/step - loss: 1.7935 - val_loss: 0.0258 - learning_rate: 0.0010\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 546ms/step - loss: 1.5613 - val_loss: 0.0255 - learning_rate: 0.0010\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:58:54.181994: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-10-21 09:58:54.182015: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "2024-10-21 09:58:54.182023: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 1.2968 - val_loss: 0.0256 - learning_rate: 0.0010\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:58:54.749458: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-10-21 09:58:54.749481: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 487ms/step - loss: 1.1518 - val_loss: 0.0258 - learning_rate: 0.0010\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 450ms/step - loss: 0.9550"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:58:56.943969: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2024-10-21 09:58:57.124772: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "2024-10-21 09:58:57.124795: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - loss: 0.9550 - val_loss: 0.0258 - learning_rate: 0.0010\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 495ms/step - loss: 0.9325 - val_loss: 0.0266 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:58:58.840624: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "2024-10-21 09:58:58.840647: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 0.8977 - val_loss: 0.0268 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:58:59.506671: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "2024-10-21 09:58:59.506712: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 552ms/step - loss: 0.8974 - val_loss: 0.0275 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:59:01.361780: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - loss: 0.9267"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:59:01.724832: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "2024-10-21 09:59:01.724851: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "2024-10-21 09:59:01.913726: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "2024-10-21 09:59:01.913752: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - loss: 0.9267 - val_loss: 0.0277 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 496ms/step - loss: 0.8864 - val_loss: 0.0283 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:59:03.627357: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2024-10-21 09:59:03.627374: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "2024-10-21 09:59:03.627383: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - loss: 0.8577 - val_loss: 0.0286 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:59:04.299576: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-10-21 09:59:04.299600: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 560ms/step - loss: 0.8691 - val_loss: 0.0293 - learning_rate: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:59:06.019402: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-10-21 09:59:06.019431: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_correlate available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fe3d9412110>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # type: ignore\n",
    "\n",
    "os.makedirs(\".chromnn\", exist_ok=True)\n",
    "callbacks_list = [\n",
    "    CSVLogger(Path(\".chromnn\", \"epoch_data.csv\")),\n",
    "    ModelCheckpoint(filepath=Path(\".chromnn\", \"Checkpoint.keras\").name, monitor=\"val_correlate\", save_best_only=True),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10, min_delta=1e-5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=6 // 2, min_lr=0.1 * 0.001),\n",
    "]\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks_list,\n",
    "    steps_per_epoch=len(bins_train) // batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown \u001b[1m3s\u001b[0m 3s/step - loss: 0.0323"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:59:58.373818: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.21GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4s/step - loss: 0.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 09:59:59.142341: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5039837812499872075\n",
      "2024-10-21 09:59:59.142361: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18235436505003952707\n",
      "/home/rsg/micromamba/envs/momics/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(test_dataset)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"chromnn_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model to predict ATAC-seq coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "momics :: INFO :: 2024-10-21 10:02:21,179 :: Query completed in 0.2572s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1848/1848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "from momics.momicsquery import MomicsQuery\n",
    "\n",
    "## Define 2048-bp-wide genomic windows, with a stride of 16 bp, and extract MNase data from it.\n",
    "bb = repo.bins(width=features_size, stride=16, cut_last_bin_out=True)[\"XVI\"]\n",
    "dat = MomicsQuery(repo, bb).query_tracks(tracks=[\"MNase_rescaled\"])\n",
    "dat = np.array(list(dat.coverage[\"MNase_rescaled\"].values()))\n",
    "dat = np.nan_to_num(dat, nan=0)\n",
    "\n",
    "## Run predictions\n",
    "predictions = model.predict(dat)\n",
    "\n",
    "## Export predictions as a bigwig\n",
    "midpoints = (bb.Start + bb.End) // 2\n",
    "bb.Start = (midpoints - target_size / 2).apply(int)\n",
    "bb.End = (midpoints + target_size / 2).apply(int)\n",
    "bb.to_bed(\"predictions_bins.bed\")\n",
    "mutils.pyranges_to_bw(bb, predictions, \"predictions_cov.bw\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
