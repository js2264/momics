{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train neural networks with `momics` and `tensorflow`\n",
    "\n",
    "`momics` provides several useful resources to train neural networks with `tensorflow`. This notebook demonstrates how to train a simple neural network with `momics` and `tensorflow`.\n",
    "\n",
    "## Connect to the data repository\n",
    "\n",
    "We will tap into the repository generated in the [previous tutorial](integrating-multiomics). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momics.momics import Momics\n",
    "\n",
    "## Creating repository\n",
    "repo = Momics(\"yeast_CNN_data.momics\")\n",
    "\n",
    "## Check that sequence and some tracks are registered\n",
    "repo.seq()\n",
    "repo.tracks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify some tracks\n",
    "\n",
    "We can first pre-process the tracks to normalize them, and save them back to the local repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for track in [\"ATAC\", \"MNase\"]:\n",
    "    cov = repo.tracks(track)\n",
    "    q99 = np.nanpercentile(np.concatenate(list(cov.values())), 99)\n",
    "    for chrom in cov.keys():\n",
    "        arr = cov[chrom]\n",
    "        arr = np.minimum(arr, q99)\n",
    "        arr = (arr - np.nanmin(arr)) / (np.nanmax(arr) - np.nanmin(arr))\n",
    "        cov[chrom] = np.nan_to_num(arr, nan=0)\n",
    "    repo.ingest_track(cov, track + \"_rescaled\")\n",
    "\n",
    "repo.tracks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define datasets and model \n",
    "\n",
    "We will define a simple convolutional neural network with `tensorflow` to predict the target variable `ATAC` from the feature variable `MNase`. This requires to first define a set of genomic coordinates to extract genomic data from. We will use `MNase_rescaled` coverage scores over tiling genomic windows (`features_size` of `1025`, with a stride of `48`) as feature variables to predict `ATAC_rescaled` coverage scores over the same tiling genomic windows, but narrowed down to the a `target_size` of `24` bp around the center of the window. We can split the data into training, testing and validation sets, using `momics.utils.split_ranges()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import momics.utils as mutils\n",
    "\n",
    "# Fetch data from the momics repository\n",
    "features = \"MNase_rescaled\"\n",
    "target = \"ATAC_rescaled\"\n",
    "features_size = 1024 + 1\n",
    "target_size = 24\n",
    "stride = 48\n",
    "\n",
    "bins = repo.bins(width=features_size, stride=stride, cut_last_bin_out=True)\n",
    "bins = bins.subset(lambda x: x.Chromosome != \"XVI\")\n",
    "bins_split, bins_test = mutils.split_ranges(bins, 0.8, shuffle=True)\n",
    "bins_train, bins_val = mutils.split_ranges(bins_split, 0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define different datasets, for training, testing and validation. We will use `momics.dataset.MomicsDataset()` constructor, indicating the batch size we wish to use in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momics.dataset import MomicsDataset\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "train_dataset = (\n",
    "    MomicsDataset(repo, bins_train, features, target, target_size=target_size, batch_size=batch_size)\n",
    "    .shuffle(10)\n",
    "    .prefetch(2)\n",
    "    .repeat()\n",
    ")\n",
    "val_dataset = MomicsDataset(repo, bins_val, features, target, target_size=target_size, batch_size=batch_size).repeat()\n",
    "test_dataset = MomicsDataset(repo, bins_test, features, target, target_size=target_size, batch_size=batch_size)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to define the model architecture. In this example, we will use a simple convolutional neural network (`ChromNN`), pre-defined in `momics.chromnn`. We can instantiate the model, and compile it with the desired optimizer, loss function and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momics.chromnn import ChromNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers  # type: ignore\n",
    "\n",
    "model = ChromNN(\n",
    "    layers.Input(shape=(features_size, 1)),\n",
    "    layers.Dense(target_size, activation=\"linear\"),\n",
    ").model\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the datasets and the model, we can fit the model to the training data, using the `fit()` method of the model. We can also evaluate the model on the testing and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # type: ignore\n",
    "\n",
    "callbacks_list = [\n",
    "    CSVLogger(Path(\".chromnn\", \"epoch_data.csv\")),\n",
    "    ModelCheckpoint(filepath=Path(\".chromnn\", \"Checkpoint.keras\").name, monitor=\"val_correlate\", save_best_only=True),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10, min_delta=1e-5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=6 // 2, min_lr=0.1 * 0.001),\n",
    "]\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks_list,\n",
    "    steps_per_epoch=len(bins_train) // batch_size,\n",
    "    validation_steps=len(bins_val) // batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and save model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the trained model performs, and save it to the local repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(test_dataset)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"chromnn_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model to predict ATAC-seq coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our trained model to predict ATAC-seq coverage from MNase-seq coverage, for example on a chromosome which has not been used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momics.query import MomicsQuery\n",
    "from momics.aggregate import aggregate\n",
    "\n",
    "## Define tiling 1025 bp windows, with a stride of 8 bp, and extract MNase data from it.\n",
    "bb = repo.bins(width=features_size, stride=8, cut_last_bin_out=True)[\"XVI\"]\n",
    "dat = MomicsQuery(repo, bb).query_tracks(tracks=[\"MNase_rescaled\"])\n",
    "dat = np.array(list(dat.coverage[\"MNase_rescaled\"].values()))\n",
    "\n",
    "## Now predict the ATAC signal from the MNase signal\n",
    "predictions = model.predict(dat)\n",
    "\n",
    "## Export predictions as a bigwig\n",
    "bb2 = bb.copy()\n",
    "bb2.Start = bb2.Start + features_size // 2 - target_size // 2\n",
    "bb2.End = bb2.Start + target_size\n",
    "chrom_sizes = {chrom: length for chrom, length in zip(repo.chroms().chrom, repo.chroms().length)}\n",
    "keys = [f\"{chrom}:{start}-{end}\" for chrom, start, end in zip(bb2.Chromosome, bb2.Start, bb2.End)]\n",
    "res = {\"atac\": {k: None for k in keys}}\n",
    "for i, key in enumerate(keys):\n",
    "    res[\"atac\"][key] = predictions[i]\n",
    "\n",
    "aggregate(res, bb2, chrom_sizes, type=\"mean\", prefix=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a new `bw` file with ATAC-seq coverage over chr16, predicted from MNase-seq coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use other feature variables for prediction\n",
    "\n",
    "We can use other predictor variable, for example the `nucleotide` sequence of tiling genomic windows, to predict `ATAC_rescaled` coverage scores. We can use the same procedure as above, but with a different set of genomic coordinates and feature variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import momics.utils as mutils\n",
    "\n",
    "for track in [\"ATAC\", \"MNase\"]:\n",
    "    cov = repo.tracks(track)\n",
    "    q99 = np.nanpercentile(np.concatenate(list(cov.values())), 99)\n",
    "    for chrom in cov.keys():\n",
    "        arr = cov[chrom]\n",
    "        arr = np.minimum(arr, q99)\n",
    "        arr = (arr - np.nanmin(arr)) / (np.nanmax(arr) - np.nanmin(arr))\n",
    "        cov[chrom] = np.nan_to_num(arr, nan=0)\n",
    "    repo.ingest_track(cov, track + \"_rescaled\")\n",
    "\n",
    "repo.tracks()\n",
    "\n",
    "\n",
    "# Fetch data from the momics repository\n",
    "features = \"MNase_rescaled\"\n",
    "target = \"ATAC_rescaled\"\n",
    "features_size = 1024 + 1\n",
    "target_size = 24\n",
    "stride = 48\n",
    "\n",
    "bins = repo.bins(width=features_size, stride=stride, cut_last_bin_out=True)\n",
    "bins = bins.subset(lambda x: x.Chromosome != \"XVI\")\n",
    "bins_split, bins_test = mutils.split_ranges(bins, 0.8, shuffle=True)\n",
    "bins_train, bins_val = mutils.split_ranges(bins_split, 0.8, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
