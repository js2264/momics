{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train neural networks with `momics` and `tensorflow`\n",
    "\n",
    "`momics` provides several useful resources to train neural networks with `tensorflow`. This notebook demonstrates how to train a simple neural network with `momics` and `tensorflow`.\n",
    "\n",
    "## Connect to the data repository\n",
    "\n",
    "We will tap into the repository generated in the [previous tutorial](integrating-multiomics.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momics import momics as mmm\n",
    "\n",
    "## Creating repository\n",
    "repo = mmm.Momics(\"yeast_CNN_data.momics\")\n",
    "\n",
    "## Check that sequence and some tracks are registered\n",
    "repo.seq()\n",
    "repo.tracks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify some tracks\n",
    "\n",
    "We can first pre-process the tracks to normalize them, and save them back to the local repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for track in [\"atac\", \"mnase\", \"scc1\"]:\n",
    "    cov = repo.tracks(track)\n",
    "    # Compute genome-wide 99th percentile\n",
    "    q99 = np.nanpercentile(np.concatenate(list(cov.values())), 99)\n",
    "    for chrom in cov.keys():\n",
    "        arr = cov[chrom]\n",
    "        # Truncate to genome-wide 99th percentile\n",
    "        arr = np.minimum(arr, q99)\n",
    "        # Rescale to [0, 1]\n",
    "        arr = (arr - np.nanmin(arr)) / (np.nanmax(arr) - np.nanmin(arr))\n",
    "        # Convert NaNs to 0\n",
    "        arr = np.nan_to_num(arr, nan=0)\n",
    "        # Store back\n",
    "        cov[chrom] = arr\n",
    "    repo.ingest_track(cov, track + \"_rescaled\")\n",
    "\n",
    "repo.tracks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define datasets and model \n",
    "\n",
    "We will define a simple convolutional neural network with `tensorflow` to predict the target variable `ATAC` from the feature variable `MNase`. This requires to first define a set of genomic coordinates to extract genomic data from. We will use `MNase_rescaled` coverage scores over tiling genomic windows (`features_size` of `1025`, with a stride of `48`) as feature variables to predict `ATAC_rescaled` coverage scores over the same tiling genomic windows, but narrowed down to the a `target_size` of `24` bp around the center of the window. We can split the data into training, testing and validation sets, using `momics.utils.split_ranges()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import momics.utils as mutils\n",
    "\n",
    "# Fetch data from the momics repository\n",
    "features_size = 8192 + 1\n",
    "stride = 48\n",
    "\n",
    "bins = repo.bins(width=features_size, stride=stride, cut_last_bin_out=True)\n",
    "bins = bins.subset(lambda x: x.Chromosome != \"XVI\")\n",
    "bins_split, bins_test = mutils.split_ranges(bins, 0.8, shuffle=False)\n",
    "bins_train, bins_val = mutils.split_ranges(bins_split, 0.8, shuffle=False)\n",
    "bins_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define different datasets, for training, testing and validation. We will use `momics.dataset.MomicsDataset()` constructor, indicating the batch size we wish to use in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momics import dataset as mmd\n",
    "\n",
    "features = [\"mnase_rescaled\", \"nucleotide\"]\n",
    "target = \"atac_rescaled\"\n",
    "target_size = 512\n",
    "batch_size = 500\n",
    "\n",
    "train_dataset = (\n",
    "    mmd.MomicsDataset(repo, bins_train, features, target, target_size=target_size, batch_size=batch_size)\n",
    "    .shuffle(10)\n",
    "    .prefetch(2)\n",
    "    .repeat()\n",
    ")\n",
    "val_dataset = mmd.MomicsDataset(repo, bins_val, features, target, target_size=target_size, batch_size=batch_size)\n",
    "test_dataset = mmd.MomicsDataset(repo, bins_test, features, target, target_size=target_size, batch_size=batch_size)\n",
    "train_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to define the model architecture. In this example, we will use a simple customizable convolutional neural network (`ChromNN`), provided in `momics.nn`. We can instantiate the model with the number and shape of layers we want, and compile it with the desired optimizer, loss function and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momics import nn\n",
    "import tensorflow as tf  # type: ignore\n",
    "from tensorflow.keras import layers  # type: ignore\n",
    "\n",
    "## Define the model with three convolutional layers\n",
    "model = nn.ChromNN(\n",
    "    inputs={\n",
    "        \"mnase_rescaled\": layers.Input(shape=(features_size, 1), name=\"mnase_rescaled\"),\n",
    "        \"nucleotide\": layers.Input(shape=(features_size, 4), name=\"nucleotide\"),\n",
    "    },\n",
    "    outputs={\"atac_rescaled\": layers.Dense(target_size, activation=\"linear\", name=\"atac_rescaled\")},\n",
    "    filters=[64, 16, 8],\n",
    "    kernel_sizes=[3, 8, 80],\n",
    ").model\n",
    "\n",
    "\n",
    "## Use a combination of MAE and correlation as loss function\n",
    "def loss_atac(y_true, y_pred):\n",
    "    return nn.mae_cor(y_true, y_pred, alpha=0.9)\n",
    "\n",
    "\n",
    "## Use Adam optimizer, a learning rate of 0.001, and return MAE as metric\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        \"atac_rescaled\": loss_atac,\n",
    "    },\n",
    "    loss_weights={\n",
    "        \"atac_rescaled\": 1.0,\n",
    "    },\n",
    "    metrics={\n",
    "        \"atac_rescaled\": \"mae\",\n",
    "    },\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the datasets and the model, we can fit the model to the training data, using the `fit()` method of the model. We can also evaluate the model on the testing and validation datasets. Here, we'll quickly iterate over 10 epochs, but you can increase this number to improve the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # type: ignore\n",
    "\n",
    "os.makedirs(\".chromnn\", exist_ok=True)\n",
    "callbacks_list = [\n",
    "    CSVLogger(Path(\".chromnn\", \"epoch_data.csv\")),\n",
    "    ModelCheckpoint(filepath=Path(\".chromnn\", \"Checkpoint.keras\"), monitor=\"val_loss\", save_best_only=True),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=40, min_delta=1e-5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=6 // 2, min_lr=0.1 * 0.001),\n",
    "]\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=30,\n",
    "    steps_per_epoch=int(np.floor(len(bins_train) // batch_size)),\n",
    "    callbacks=callbacks_list,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and save model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the trained model performs, and save it to the local repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with our test dataset\n",
    "model.evaluate(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model to predict ATAC-seq coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momics import aggregate as mma\n",
    "\n",
    "## Predict the ATAC signal from the MNase signal\n",
    "bb = repo.bins(width=features_size, stride=8, cut_last_bin_out=True)[\"XVI\"]\n",
    "ds = mmd.MomicsDataset(repo, bb, features, batch_size=1000).prefetch(10)\n",
    "predictions = model.predict(ds)\n",
    "\n",
    "## Export predictions as a bigwig\n",
    "centered_bb = bb.copy()\n",
    "centered_bb.Start = centered_bb.Start + features_size // 2 - target_size // 2\n",
    "centered_bb.End = centered_bb.Start + target_size\n",
    "chrom_sizes = repo.chroms(as_dict=True)\n",
    "keys = [f\"{chrom}:{start}-{end}\" for chrom, start, end in zip(centered_bb.Chromosome, centered_bb.Start, centered_bb.End)]\n",
    "res = {f\"atac-from-seq-and-mnase_f{features_size}_s{stride}_t{target_size}\": {k: None for k in keys}}\n",
    "for i, key in enumerate(keys):\n",
    "    res[f\"atac-from-seq-and-mnase_f{features_size}_s{stride}_t{target_size}\"][key] = predictions[\"atac_rescaled\"][i]\n",
    "\n",
    "mma.aggregate(res, centered_bb, chrom_sizes, type=\"mean\", prefix=\"prediction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a new `bw` file with ATAC-seq coverage over chr16, predicted from MNase-seq coverage.\n",
    "\n",
    "Here is a screenshot of ATAC-seq coverage track over chr16, from experimental data (darker cyan) or predicted from MNase-seq coverage (MNase: grey track; predicted ATAC: lighter cyan), taken from IGV:\n",
    "\n",
    "![ATAC-seq coverage track over chr16](images/atac_mnase.png)\n",
    "\n",
    "A closer look: \n",
    "\n",
    "![ATAC-seq coverage track over chr16, zoom](images/atac_mnase2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of the predicted and experimental ATAC-seq coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
